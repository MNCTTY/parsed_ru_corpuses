{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "import wget\n",
    "import tensorflow as tf\n",
    "import kashgari\n",
    "from kashgari.embeddings import BERTEmbedding\n",
    "from kashgari.tasks.classification import BiLSTM_Model\n",
    "from kashgari.utils import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "from zipfile import ZipFile\n",
    "warnings.simplefilter(\"ignore\", category=PendingDeprecationWarning)\n",
    "\n",
    "from keras import backend\n",
    "print(\"Is gpu available:\", tf.test.is_gpu_available())\n",
    "print(\"Available gpus:\", backend.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_preprocess(data):\n",
    "    tokenizer = bert_embedding.tokenizer\n",
    "    sentences_tokenized = []\n",
    "    for sentence in data:\n",
    "        sentence_tokenized = tokenizer.tokenize(sentence)\n",
    "        sentences_tokenized.append(sentence_tokenized)\n",
    "    return sentences_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/mnt/sdc/'\n",
    "logging.basicConfig(level='DEBUG')\n",
    "bert_model_name='google-bert-multi'\n",
    "bert_model_path = root_dir + 'bert_model/'\n",
    "bert_embedding = BERTEmbedding(bert_model_path, sequence_length=300, task=kashgari.CLASSIFICATION)\n",
    "model = BiLSTM_Model(bert_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучим классификатор ошибочного употребления тся/ться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend\n",
    "print(\"Is gpu available:\", tf.test.is_gpu_available())\n",
    "print(\"Available gpus:\", backend.tensorflow_backend._get_available_gpus())\n",
    "\n",
    "root_dir = '/home/orpho/data'\n",
    "\n",
    "train_data = pd.read_csv(root_dir + '/train_data_bklif.csv', index_col=0)\n",
    "val_data = pd.read_csv(root_dir + '/val_data_bklif.csv', index_col=0)\n",
    "print(\"Train and val data shape\", train_data.shape, val_data.shape)\n",
    "\n",
    "logging.basicConfig(level='DEBUG')\n",
    "\n",
    "\"\"\"## Train: simple biLSTM\n",
    "### Define model\n",
    "\"\"\"\n",
    "\n",
    "bert_model_name='google-bert-multi'\n",
    "bert_model_path = root_dir + '/models/'\n",
    "\n",
    "if not os.path.exists(bert_model_path+bert_model_name):\n",
    "    if not os.path.exists(bert_model_path):\n",
    "        os.mkdir(bert_model_path)\n",
    "    bert_url = 'https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip'\n",
    "    wget.download(bert_url, out=bert_model_path+bert_model_name+'.zip')\n",
    "    zipdata = ZipFile(bert_model_path+bert_model_name+'.zip')\n",
    "    zipinfos = zipdata.infolist()\n",
    "    for zipinfo in zipinfos:\n",
    "        zipinfo.filename = zipinfo.filename.replace('multi_cased_L-12_H-768_A-12', bert_model_name)\n",
    "        zipdata.extract(zipinfo, path=bert_model_path)\n",
    "    print(\"Unzipped\")\n",
    "\n",
    "bert_model_path += bert_model_name\n",
    "bert_embedding = BERTEmbedding(bert_model_path, sequence_length=300, task=kashgari.CLASSIFICATION)\n",
    "model = BiLSTM_Model(bert_embedding)\n",
    "\n",
    "\"\"\"### Data generation\"\"\"\n",
    "\n",
    "print(pd.Series(train_data['y']).value_counts())\n",
    "\n",
    "x_train_preprocessed = bert_preprocess(train_data['x'])\n",
    "x_val_preprocessed = bert_preprocess(val_data['x'])\n",
    "bert_embedding.processor.add_bos_eos = False\n",
    "\n",
    "y_train = list(map(bool, train_data['y']))\n",
    "y_val = list(map(bool, val_data['y']))\n",
    "print(\"Len of current training data\", len(y_train), len(x_train_preprocessed))\n",
    "print(type(y_train[0]))\n",
    "\n",
    "\"\"\"### Train model\n",
    "\n",
    "#### Train from checkpoint\n",
    "\"\"\"\n",
    "train_from_chkpt = False\n",
    "multi_gpu = False\n",
    "test_mode = False\n",
    "model_name = 'bilstm_with_vocab_bklif_from_scratch'\n",
    "model_save_dir = root_dir + '/trained/'\n",
    "if not os.path.exists(root_dir + '/trained'):\n",
    "    os.mkdir(root_dir + '/trained')\n",
    "\n",
    "chkpt_callback = tf.keras.callbacks.ModelCheckpoint(root_dir+'/chkpt/weights_bklif.{epoch:02d}-{acc:.2f}-{val_acc:.2f}.hdf5',\n",
    "                        monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto')\n",
    "\n",
    "chkpt_dir = root_dir + '/chkpt/'\n",
    "\n",
    "if test_mode:\n",
    "    model = load_model(root_dir+'/trained/' + model_name, load_weights=True)\n",
    "    replicas = np.array(['Надо чаще встречаться', 'Надо чаще встречатся', 'Собака не шевелится', 'Собака не шевелиться',\n",
    "            'Собака не хочет шевелится', 'Собака не хочет шевелиться', 'Он всё равно со всеми перессорится.',\n",
    "            'Он всё равно со всеми перессориться.', 'Не дай ему там со всеми перессориться.',\n",
    "            'Не дай ему там со всеми перессорится.'])\n",
    "    replicas_preprocessed = bert_preprocess(replicas)\n",
    "    for i, s in enumerate(replicas_preprocessed):\n",
    "        print(replicas[i], model.predict([s]))\n",
    "else:\n",
    "    if train_from_chkpt:\n",
    "        if not os.path.exists(root_dir + '/chkpt'):\n",
    "            raise Exception(\"Checkpoint does not exist\")\n",
    "        else:\n",
    "            chkpt_path = chkpt_dir + 'weights2.06-0.68.hdf5'\n",
    "            model.build_model(x_train_preprocessed, y_train, x_val_preprocessed, y_val)\n",
    "            model.tf_model.load_weights(chkpt_path)\n",
    "            model.fit(x_train_preprocessed, y_train, x_val_preprocessed, y_val,\n",
    "                epochs = 1,\n",
    "                batch_size = 1024,\n",
    "                callbacks = [chkpt_callback],\n",
    "                fit_kwargs = {'initial_epoch': 0})\n",
    "    else:\n",
    "        if not os.path.exists(root_dir + '/chkpt'):\n",
    "            os.mkdir(root_dir + '/chkpt')\n",
    "        if multi_gpu:\n",
    "            model.build_multi_gpu_model(self,\n",
    "                gpus=2,\n",
    "                x_train=x_train_preprocessed,\n",
    "                y_train= y_train,\n",
    "                x_validate=x_val_preprocessed,\n",
    "                y_validate=y_val)\n",
    "        model.fit(x_train_preprocessed, y_train, x_val_preprocessed, y_val,\n",
    "                epochs=10,\n",
    "                batch_size=1024,            \n",
    "                callbacks=[chkpt_callback])\n",
    "\n",
    "    model.save(model_save_dir + model_name)\n",
    "\n",
    "    replicas = np.array(['Надо чаще встречаться', 'Надо чаще встречатся', 'Собака не шевелится', 'Собака не шевелиться',\n",
    "                'Собака не хочет шевелится', 'Собака не хочет шевелиться', 'Он всё равно со всеми перессорится.',\n",
    "                'Он всё равно со всеми перессориться.', 'Не дай ему там со всеми перессориться.',\n",
    "                'Не дай ему там со всеми перессорится.'])\n",
    "    replicas_preprocessed = bert_preprocess(replicas)\n",
    "    for i, s in enumerate(replicas_preprocessed):\n",
    "        print(replicas[i], model.predict([s]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Воспользуемся полученной моделью, чтобы сделать корректор ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(root_dir+'lstm_model/', load_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = np.array(['Что-то мне не хочеться играть сегодня, а должно хотеться'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Реплики, на которых Саша тестирует\n",
    "replicas = np.array(['Надо чаще встречаться', 'Надо чаще встречатся', 'Собака не шевелится', 'Собака не шевелиться',\n",
    "            'Собака не хочет шевелится', 'Собака не хочет шевелиться', 'Он всё равно со всеми перессорится.',\n",
    "            'Он всё равно со всеми перессориться.', 'Не дай ему там со всеми перессориться.',\n",
    "            'Не дай ему там со всеми перессорится.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___ для одномерного случая есть ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicas = np.array(['Надо чаще встречатся'])\n",
    "replicas_preprocessed = bert_preprocess(replicas)\n",
    "for i, s in enumerate(replicas_preprocessed):\n",
    "    result = model.predict([s])\n",
    "    if result != ['true']:\n",
    "        while result != ['true']:\n",
    "            s = str(replicas[0])\n",
    "            words = s.split(' ')\n",
    "            for i in range(len(words)):\n",
    "                if 'тся' in words[i]:\n",
    "                    words[i] = 'ться'.join(words[i].split('тся'))\n",
    "                    #print('я')\n",
    "\n",
    "                else:\n",
    "                    if 'ться' in words[i]:\n",
    "                        words[i] = 'тся'.join(words[i].split('ться')) \n",
    "                        #print('й')\n",
    "\n",
    "            s = ' '.join(words)\n",
    "            replicas = np.array([s])\n",
    "\n",
    "            replicas_preprocessed = bert_preprocess(replicas)\n",
    "            for i, s in enumerate(replicas_preprocessed):\n",
    "                result = model.predict([s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_ для многомерного случая есть _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicas = np.array(['Надо чаще встречатся, смеятся'])\n",
    "replicas_preprocessed = bert_preprocess(replicas)\n",
    "for i, s in enumerate(replicas_preprocessed):\n",
    "    result = model.predict([s])\n",
    "    if result != ['true']:\n",
    "        while result != ['true']:\n",
    "            s = str(replicas[0])\n",
    "            words = s.split(' ')\n",
    "            \n",
    "            ###\n",
    "            suspended = []\n",
    "            idx = []\n",
    "            for w in words:\n",
    "                if 'тся' in w:\n",
    "                    suspended.append(w)\n",
    "                    idx.append(words.index(w))\n",
    "                    \n",
    "                if 'ться' in w:\n",
    "                    suspended.append(w)\n",
    "                    idx.append(words.index(w))\n",
    "                    \n",
    "                    \n",
    "            \n",
    "            BITSIZE = len(suspended)\n",
    "\n",
    "            for y in range(1<<BITSIZE):\n",
    "                a = y\n",
    "                b = 0\n",
    "                for k in range(BITSIZE):\n",
    "                    b <<= 1\n",
    "                    b ^= i&1\n",
    "                    y >>= 1\n",
    "                current = '{:010b}'.format(a, b)[-BITSIZE:]\n",
    "\n",
    "                for j in range(BITSIZE):\n",
    "                    \n",
    "                    if 'тся' in suspended[j]:\n",
    "                        q = suspended[j].split('тся')\n",
    "                        if current[j] == 1:\n",
    "                            suspended[j] = 'тся'.join(q)\n",
    "                            #print('1')\n",
    "                        else: \n",
    "                            suspended[j] = 'ться'.join(q)\n",
    "                            #print('2')\n",
    "\n",
    "                    else:\n",
    "                        q = suspended[j].split('ться')\n",
    "                        if current[j] == 1:\n",
    "                            suspended[j] = 'тся'.join(q)\n",
    "                            #print('3')\n",
    "                        else: \n",
    "                            suspended[j] = 'ться'.join(q)\n",
    "                            #print('4')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #склеить\n",
    "                counter = 0\n",
    "                for j in idx:\n",
    "                    words[j] = suspended[counter]\n",
    "                    counter = counter +1\n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "            ###\n",
    "            #проверка\n",
    "            \n",
    "            s = ' '.join(words)\n",
    "            replicas = np.array([s])\n",
    "\n",
    "            replicas_preprocessed = bert_preprocess(replicas)\n",
    "            for i, s in enumerate(replicas_preprocessed):\n",
    "                result = model.predict([s])\n",
    "#print('хай')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
