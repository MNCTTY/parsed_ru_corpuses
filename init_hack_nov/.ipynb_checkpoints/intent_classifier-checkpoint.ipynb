{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('SELECT', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.rstrip()\n",
    "        data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Почему когда я пытаюсь войти с телефона в полную версию интернет банка меня автоматически перекидывет в мобильное приложение,Интернет-банк',\n",
       " '\"Невозможно войти в интернет банк, не приходят смс с паролем\",Интернет-банк',\n",
       " 'да,Интернет-банк',\n",
       " 'смс не приходят,Банковские карты',\n",
       " 'Дальше,Банковские карты']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Невозможно войти в интернет банк',\n",
       " ' не приходят смс с паролем\"',\n",
       " 'Интернет-банк']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Интернет-банк'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].split(',')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Невозможно войти в интернет банк, не приходят смс с паролем\"'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "','.join(data[1].split(',')[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for i in range(0, len(data)):\n",
    "    string = ','.join(data[i].split(',')[:-1])\n",
    "    label = data[i].split(',')[-1]\n",
    "    df_list.append([string, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['да', 'Интернет-банк']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df_list, columns=['string', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>14588</td>\n",
       "      <td>оператор</td>\n",
       "      <td>Соедините с оператором</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14589</td>\n",
       "      <td>оператор</td>\n",
       "      <td>Соедините с оператором</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14590</td>\n",
       "      <td>оператор</td>\n",
       "      <td>Соедините с оператором</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14591</td>\n",
       "      <td>оператор</td>\n",
       "      <td>Соедините с оператором</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14592</td>\n",
       "      <td>оператор</td>\n",
       "      <td>Соедините с оператором</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         string                   label\n",
       "14588  оператор  Соедините с оператором\n",
       "14589  оператор  Соедините с оператором\n",
       "14590  оператор  Соедините с оператором\n",
       "14591  оператор  Соедините с оператором\n",
       "14592  оператор  Соедините с оператором"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.groupby(['label'], as_index=False, sort=False).agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Почему когда я пытаюсь войти с телефона в полную версию интернет банка меня автоматически перекидывет в мобильное приложение',\n",
       "  'Интернет-банк'],\n",
       " ['\"Невозможно войти в интернет банк, не приходят смс с паролем\"',\n",
       "  'Интернет-банк'],\n",
       " ['да', 'Интернет-банк'],\n",
       " ['смс не приходят', 'Банковские карты'],\n",
       " ['Дальше', 'Банковские карты'],\n",
       " ['почему я не могу перевести балы ярко на карту', 'Бонусная программа'],\n",
       " ['балы ярко не списать на карту', 'Бонусная программа'],\n",
       " ['как узнать подключена у меня программа ярко', 'Бонусная программа'],\n",
       " ['как перевести балл ярко на карту ', 'Бонусная программа'],\n",
       " ['при переводе бонусов ярко на карту в разделе возращаемые операции ничего не отображается',\n",
       "  'Бонусная программа']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_data = []\n",
    "current_text = df_list[0][0]\n",
    "current_label = df_list[0][1]\n",
    "for i in range(1, len(df_list)):\n",
    "    next_label = df_list[i][1]\n",
    "    if next_label != current_label:\n",
    "        agg_data.append([current_text, current_label])\n",
    "        current_label = next_label\n",
    "        current_text = df_list[i][0]\n",
    "    else:\n",
    "        current_text += df_list[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала сделаем необходимые импорты библиотек и подгрузим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = подгрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем данные на данные для обучения и для теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Text', 'TotalWords']]\n",
    "Y = df['Label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это необязательный шаг, но добавляет исследованию решения структурности.\n",
    "\n",
    "Построим пайплайн, который включает в себя все последующие подходы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('colext', TextSelector('Text')),\n",
    "            ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, stop_words=stop_words,\n",
    "                     min_df=.0025, max_df=0.25, ngram_range=(1,3))),\n",
    "            ('svd', TruncatedSVD(algorithm='randomized', n_components=300)), #for XGB\n",
    "        ])),\n",
    "        ('words', Pipeline([\n",
    "            ('wordext', NumberSelector('TotalWords')),\n",
    "            ('wscaler', StandardScaler()),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.1)),\n",
    "#    ('clf', RandomForestClassifier()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем FeatureUnion потому что это позволит нам объединить пайплайны, которые мы запустим на разных фичах обучающих данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый пайплайн функций начинается с трансформера, выбирающего конкретную фичу. Трансформеры должны реализовывать только методы Transform и Fit. Ниже представлены те, которые будем использовать для извлечения столбцов данных (обратите внимание, они отличаются для текстовых и числовых данных):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.field]\n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[[self.field]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы обрабатываем числовые столбцы с помощью StandardScaler, который стандартизирует данные, удаляя среднее значение и масштабируя до единицы дисперсии. Это общее требование для классификаторов машинного обучения. Большинство из них не будет вести себя так, как ожидалось, если отдельные фичи не похожи на стандартно нормально распределенные данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tfidf-Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка текста является невероятно важной подзадачей, так как именно там находится большая часть интересующих нас фичей. В данном бейзлайне будем использовать TfidfVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-Idf учитывает частоту слова в документе, взвешенную по тому, как часто оно появляется во всем корпусе. Общие слова, такие как the или that, будут иметь высокие tf, но когда они взвесятся по idf, получим 1 (эти слова появляются в каждом документе), и поскольку TfIdf использует значения логарифм значения, этот вес фактически будет равен 0, поскольку log 1 = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для сравнения, если какой-то документ содержит слово \"математика\", и это единственный документ по этой теме из набора 100 документов, то обратная частота будет равна 100, поэтому его значение Tf-Idf будет увеличено, что означает, что документ однозначно связан с темой \"математика\". TfidfVectorizer в sklearn возвращает матрицу с tf-idf каждого слова в каждом документе, с более высокими значениями для слов, специфичных для этого документа, и низкими (0) значениями для слов, которые появляются по всему корпусу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы можете поперебирать параметры, используя GridSearch или другие оптимизаторы гиперпараметров. Текущие параметры обозначают: \n",
    "- мы выбираем n-граммы в диапазоне (1,3): отдельные слова, биграммы и триграммы \n",
    "- ограничиваем nграммы частотой распределения по корпусу между ними .0025 и .25\n",
    "- и мы используем пользовательский токенизатор, который извлекает только числовые и буквенные слова и применяет stemmer. \n",
    "\n",
    "То, что делает стеммер, сводит флективные формы и деривационно связанные формы слова к общей базовой форме, поэтому он уменьшает пространство признаков. Например, Стеммер Портера мы используем здесь бы уменьшить “говорю”, “говорю”, “говорит” или “говорит” просто “говорят”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def Tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    porter_stemmer=nltk.PorterStemmer()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После построения векторного представления текста, выбираем классификатор. Если мы используем XGBoost, то  нужно добавить трансформатор TruncatedSVD в конвейер. Его роль заключается в выполнении линейного уменьшения размерности с помощью усеченного сингулярного разложения (SVD). Он работает на матрицах tf-idf, созданных sklearn, делая то, что называется скрытым семантическим анализом (LSA).  Это нужно для работы XGBoost, поскольку он не принимает разреженные матрицы. \n",
    "\n",
    "Для других классификаторов вы можете просто закомментить его."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost - один из самых мощных  классификаторов, прославленный своей цепочкой побед в соревнованиях на Kaggle. Вы можете попробовать другие, не стесняйтесь экспериментировать! RandomForestClassifier и LinearSVC также очень неплохи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost‑0.71‑cp27‑cp27m‑win_amd64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost предлагает несколько дополнительных функций для настройки моделей, вычислительных сред и улучшения алгоритмов.\n",
    "Он способен выполнять три основные формы градиентного бустинга (градиентный бустинг (GB), стохастический GB и Регуляризованный GB) и достаточно надежен для файн-тюнинга и добавления параметров регуляризации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)\n",
    "preds = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ производительности классификатора является сложной статистической задачей. Здесь ярассмотрим некоторые наиболее распространенные показатели качества, используемые для быстрой оценки результатаЭ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто для оценки результатов используют метрику accuracy. \n",
    "Иногда это не самый лучший вариант, и стоит выбрать метрики precision и recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix\n",
    "print \"Accuracy:\", accuracy_score(y_test, preds)\n",
    "print \"Precision:\", precision_score(y_test, preds)\n",
    "print classification_report(y_test, preds)\n",
    "print confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем другой подход"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert + knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.models import Model, load_model\n",
    "from keras import backend as K\n",
    "from keras_radam import RAdam\n",
    "from keras_bert import Tokenizer\n",
    "import logging\n",
    "import joblib\n",
    "import codecs\n",
    "logging.basicConfig(level='DEBUG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения векторного представления текста будем использовать гораздо более сложную, предобученную на большом количествче данных, архитектуру - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '.' \n",
    "pretrained_path = root_dir + '/models/google-bert-multi/'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')\n",
    "chkpt_dir = root_dir + '/chkpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 42\n",
    "SEQ_LEN = 512\n",
    "EMB_SIZE = 768\n",
    "LR = 1e-5\n",
    "DATA_COLUMN = 'message'\n",
    "LABEL_COLUMN = 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "old_data['y'] = label_encoder.fit_transform(list(old_data['rubric']))\n",
    "new_data['y'] = label_encoder.transform(list(new_data['rubric']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "получаем эмбеддинги (векторные представления) текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "\n",
    "with BertClient(port=5555, port_out=5556, check_version=False) as bc:\n",
    "    new_encodings = np.array(bc.encode(list(new_data['message'])))\n",
    "    pd.DataFrame(new_encodings).to_csv('data/' + 'new_data_' + 'encodings.csv')\n",
    "\n",
    "with BertClient(port=5555, port_out=5556, check_version=False) as bc:\n",
    "    old_encodings = np.array(bc.encode(list(old_data['message'])))\n",
    "    pd.DataFrame(old_encodings).to_csv('data/' + 'old_data_' + 'encodings.csv')\n",
    "\n",
    "# в другом процессе из терминала было предварительно запущено:\n",
    "# bert-serving-start -num_worker=4 -max_seq_len 512 -model_dir models/google-bert-multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(pd.read_csv(root_dir + 'data/' + 'old_data_' + 'encodings.csv', index_col=0))\n",
    "y_train = np.array(old_data['y'])\n",
    "x_test = np.array(pd.read_csv(root_dir + 'data/' + 'new_data_' + 'encodings.csv', index_col=0))\n",
    "y_test = np.array(new_data['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве классификатора будем использовать k nearest neighbours - простейший метрический классификатор, основанный на оценивании сходства объектов. Классифицируемый объект относится к тому классу, которому принадлежат ближайшие к нему объекты обучающей выборки.\n",
    "\n",
    "Его недостатком является значительное снижение скорости работы при увеличении количества данных. При этом, для устранения этой проблемы можнео использовать различные подходы - от выбора алгоритма того, каким образом просматриваются соседи (например, kd_tree) и подбора гиперпараметров, до использования оптимизированной реализации на плюсах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При k=1 алгоритм ближайшего соседа неустойчив к шумовым выбросам: он даёт ошибочные классификации не только на самих объектах-выбросах, но и на ближайших к ним объектах других классов. При k=m, наоборот, алгоритм чрезмерно устойчив и вырождается в константу. Таким образом, крайние значения k нежелательны. На практике оптимальное значение параметра k определяют по критерию скользящего контроля, чаще всего — методом исключения объектов по одному (leave-one-out cross-validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=15, weights='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если на обучение тратится большое количество времени, рекоммендуется время от времени сохранять модель, чтобы затем подгружать чекпойнты из файла для дальнейшего обучения или использования (скоринга/предсказания)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('knnPickle', 'wb') as file:\n",
    "    pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('knnPickle', 'rb') as file:\n",
    "    loaded_knn = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_knn.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
